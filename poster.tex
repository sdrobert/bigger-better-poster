% !TEX engine = pdflatex

\documentclass[
  gradient, landscape,
  style=bow,
  leftlogo=figs/05_Uniligual_FullColour_Horizontal.eps,
  logo=figs/DSI_Colour_Signature_Lock-Up_Screen.png,
]{utposter}

\usepackage{mathtools}
\usepackage{url}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{amssymb,amsmath}
\usepackage{booktabs,multirow}
\geometry{a0paper}

\def\rot#1{\rotatebox{90}{#1}}

\newcolumntype{Y}{>{\centering\arraybackslash}X}


\def\x{{\mathbf x}}
\def\istar{{\mathbf i}^*}
\def\z{{\mathbf z}}
\def\tz{{\widetilde z}}
\def\a{{\mathbf a}}
\def\b{{\mathbf b}}
\def\c{{\mathbf c}}
\def\v{{\mathbf v}}
\def\L{{\cal L}}
\def\R{{\mathbb R}}
\def\N{{\mathbb N}}

% \makeatletter
% \input{fonts/fontfile26pt.clo}
% \makeatother

\title{\uoftheader{Bigger is Not Always Better: The Effect of Context Size on Speech Pre-Training}}
\institute{University of Toronto}
\author{Sean Robertson, Ewan Dunbar}
\email{sdrobert@cs.toronto.edu,ewan.dunbar@utoronto.ca}

\begin{document}

\maketitle[width=.5\textwidth]

\begin{columns}

\column{0.33}

\block{\uoftheader{Motivations}}{%
  \begin{itemize}
    \item Some speech processing tasks can be performed on less than a
          second of input; others need much more.%
    \item In the supervised setting (\textit{e.g.} ASR), excess is OK: the
          network can learn to filter out irrelevant context.%
    \item Pre-trained speech models uncritically adopt the ``more is better''
          mantra of their supervised cousins, but how do they know what's
          irrelevant?
  \end{itemize}
  \vspace{1em}
  \coloredbox{%
    \textbf{Hypothesis:} Providing too much input as context to a pre-trained
    speech model is detrimental to phoneme discriminability.%
  }
}

\block{\uoftheader{Phoneme Discriminability}}{%
  \begin{itemize}
    \item Phonemes are defined as the minimal acoustic unit of speech such that
    changing the identity of one changes the identity of the containing word.
    \item Though additional context helps humans distinguish between phonemes,
    we can do so with high accuracy with only a fraction of a second's
    exposure.
    \item Pre-trained speech representations ought to be able to do the same!
    \item ABX error rates estimate the frequency with which pairs of spans of
    speech features are more dissimilar \emph{within} phoneme classes than
    \emph{between}.
    \item Formally, for sets of all spans of speech features $A, B$ aligned to
    unique phonemes (of $N$ possible phonemes), the ABX error rate is
    approximated as:
    %
    \begin{gather*}
      ABX \approx 1 - \frac{1}{N(N-1)} \sum_{A,B} \Delta(A, B),\>\textrm{where}\\
      \Delta(A,B) = \frac{
          \sum_{{\bf a} \in A} \sum_{{\bf b} \in B} \sum_{\x \in A\setminus\{{\bf a}\}}
            I[d({\bf a},\x) < d({\bf b},\x)] +
              \frac{1}{2} I[d({\bf a},\x) = d({\bf b},\x)]
        }{
          |A|(|A| - 1)|B|
        }
    \end{gather*}
    \item $d({\bf a}, {\bf x})$ is computed with Dynamic Time warping over
    spans $\bf a$ and $\x$.
  \end{itemize}
}

\block{\uoftheader{Limiting Context}}{%
  \begin{itemize}
    \item To limit the context available to speech representations without
    changing the size of our models, we implement \emph{causal, chunked
    self-attention}.
    \item Define query vector $q \in \R^{d_Q}$, key vectors ${\bf k}$, $k_t \in
    \R^{d_K}$, and the same number of value vectors ${\bf v}$, $v_t \in
    \R^{d_V}$. Parametrized by a score function $\mathord{score} : \R^{d_Q}
    \times \R^{d_K} \to \R$, \emph{attention} produces a vector in $\R^{d_V}$
    defined as:
    %
    \begin{gather*}
      \mathord{attend}(q, {\bf k}, {\bf v}) = \sum_{t=1}^T \alpha(q, {\bf k}) v_t,\mathrm{\>where} \label{eq:att} \\
      \alpha(q, {\bf k}) = \frac{\exp\left(\mathord{score}(q, k_t)\right)}
                                {\sum_{t'=1}^T exp\left(\mathord{score}(q, k_{t'})\right)}.
    \end{gather*}
    %
    \item For fixed context width $W \in \N$ and input sequence $\x^{(in)}$,
    $x^{(in)}_t \in \R^d$, causal, chunked attention is defined as:
    %
    \begin{equation*}
      x_t^{(out)} = \mathord{attend}\left(x_t^{(in)}, \x_{\max(t-W,1):t}^{(in)}, \x_{\max(t-W,1):t}^{(in)}\right).
    \end{equation*}
  \end{itemize}
}

\column{0.33}

\block{\uoftheader{Pre-training Objective}}{%
  \begin{itemize}
    \item Pre-training is a round of unsupervised training designed to
    transform audio input $\x$ into speech representation vectors $\c$,
    $c_t \in \R^d$.
    \item \emph{Contrastive predictive coding} (CPC) learns useful
    representations $\c$ by using them to \emph{predict} latent vectors $\z$,
    $\x \mapsto \z \mapsto \c$ and \emph{contrasts} those latent vectors
    against one another.
    \item Formally, using $\c$ to generate $S$ prediction sequences, $\c
    \mapsto \v^{(s)}$, and drawing some number of ``distractor'' latent vectors
    $\widetilde{z}$ uniformly from $\z$, the CPC loss is defined as:
    %
    \begin{gather*}
      \mathcal{L}_{\text{CPC}} = \frac{1}{S}\sum_{s=1}^S \mathcal{L}^{(s)}_{\text{CPC}},\text{\>where} \label{eq:cpc} \\
      \mathcal{L}^{(s)}_{\text{CPC}} = - \frac{1}{T-S}\sum_{t=1}^{T - S} \log \frac{\exp\left(z_{t+s}^\intercal v^{(s)}_t\right)}
                                                             {\sum_{\tz} \exp\left(\tz^\intercal v^{(s)}_t\right)}.
                                                                                     \label{eq:cpc_s}
    \end{gather*}  \end{itemize}
}

\block{\uoftheader{Experiments}}{%
  \coloredbox{
    \textbf{Open source URL:} \url{https://github.com/sdrobert/scpc}
  }
  \vspace{.5em}
  \begin{itemize}
    \item Our experiments primarily measure the impact of context on ABX error
    rates.
    \item We modify the baseline CPC architecture used in the Zero Resource
    Speech Challenges:
      %
      \begin{itemize}
        \item $\x \mapsto \z$ is a 5-layer stack of dilated convolutions;
        \item $\z \mapsto \c$ is a single Transformer layer with causal,
        chunked self-attention; and
        \item $\c \mapsto \v^{(s)}$ is a single Transformer layer with causal
        (non-chunked) self-attention, which is thrown away after pre-training.
      \end{itemize}
    \item We control the context width $W$ in the causal, chunked self-attention
    layer.
    \begin{itemize}
      \item $W$ ranges between $2$ frames ($40$ms) and $128$ frames ($1300$sm).
      \item The phoneme duration in the training data (LibriSpeech) is $90 \pm
      50$ ms.
    \end{itemize}
    \item We first measure the significance of context width on ABX error rates
    \textit{via} a repeated-measures ($N=5$) one-way ANOVA.
    \item We probe the source of significance via \textit{post-hoc} Wilcoxon
    signed-rank tests.
    \item In addition, we run a series of auxiliary experiments to determine
    whether context effects are robust:
    %
    \begin{itemize}
      \item we increase the number of chunked Transformer layers
      -- \textit{2-layer} or \textit{4-layer};
      \item we replace Transformer layers with convolutions, either
      with a fixed output size -- \textit{conv (fixed $H_2$)} -- or a fixed number of
      layer parameters -- \textit{conv (fixed param)};
      \item we extend the duration of training -- \textit{long train};
      \item we increase the amount of training data -- \textit{960h};
      \item we set $\mathcal{L}_{\text{CPC}} = \mathcal{L}^{(6)}_{\text{CPC}}$
      -- \textit{last at $S = 6$}; and
      \item we replace the CPC loss with a masked prediction loss --
      \textit{BEST-RQ}.
    \end{itemize}
    \item Finally, we replicate the SUPERB ASR task on the best-performing
    models from the main setup.
  \end{itemize}
}

\column{0.33}

\block{\uoftheader{Results}}{%
  \coloredbox{%
    \textbf{Main result:} context has a significant effect on phoneme
    discriminability, with better performance on shorter windows. }
  \vspace{1em}
  \begin{center}
    ABX error rates (\%). Lower is better.
    \begin{tabularx}{0.25\columnwidth}{r | *{7}{Y}}
      \toprule
                          & \multicolumn{7}{c}{context width $W$} \\
              condition   & $2$        & $4$        & $8$        & $16$       & $32$       & $64$       & $128$ \\
       \midrule
       main               & $15.6$     & $\bf 14.2$ & $15.1$     & $15.3$     & $14.8$     & $16.9$     & $17.7$\\
       main (best)        & $15.3$     & $\bf 12.6$ & $13.3$     & $13.8$     & $13.6$     & $16.2$     & $15.9$\\\midrule
       2-layer            & $13.7$     & $15.5$     & $15.8$     & $\bf 12.6$ & $13.6$     & $16.8$     & $13.0$\\
       4-layer            & $13.8$     & $\bf 13.0$ & $14.1$     & $15.8$     & $15.8$     & $16.8$     & $17.1$\\
       conv (fixed param) & $17.0$     & $\bf 14.6$ & $16.8$     & $17.0$     & $19.0$     & $19.4$     & $20.7$\\
       conv (fixed $H_2$) & $15.0$     & $14.6$     & $16.9$     & $\bf 14.2$ & $50.0$     & $22.3$     & $43.7$\\
       long train         & $13.4$     & $\bf 13.0$ & $14.0$     & $14.4$     & $14.3$     & $14.7$     & $15.0$\\
       960h               & $-$        & $-$        & $\bf 17.2$ & $-$        & $-$        & $-$        & $18.5$\\
       last at $S=6$      & $14.0$     & $13.9$     & $\bf 13.1$ & $15.4$     & $13.3$     & $14.9$     & $16.0$\\
       BEST-RQ            & $27.6$     & $\bf 24.1$ & $24.5$     & $26.6$     & $26.1$     & $28.0$     & $25.2$\\
       % & $XX.X$     & $XX.X$     & $XX.X$     & $XX.X$     & $XX.X$     & $XX.X$     & $XX.X$\\
       \bottomrule
    \end{tabularx}
    \vspace{1em} \\
    ASR error rates (\%) with LMs. Lower is better.
    \begin{tabularx}{0.25\columnwidth}{r | *{4}{Y}}
      \toprule
                     & \multicolumn{4}{c}{partition} \\%& \multicolumn{4}{|c}{with LM} \\
       context width $W$ & dev-clean  & dev-other  & test-clean & test-other\\% & dev-clean  & dev-other  & test-clean & test-other \\
       \midrule
       $2$         & $19.1$     & $39.4$     & $18.0$     & $43.9$ \\
       $4$         & $\bf 16.5$ & $\bf 36.6$ & $\bf 15.4$ & $41.0$ \\
       $8$         & $17.0$     & $37.3$     & $16.1$     & $\bf 40.9$ \\
       $16$        & $19.4$     & $39.9$     & $17.9$     & $43.9$ \\
       $32$        & $18.1$     & $38.4$     & $17.1$     & $42.6$ \\
       $64$        & $22.7$     & $42.9$     & $21.3$     & $48.5$ \\
       $128$       & $19.9$     & $39.6$     & $18.9$     & $44.2$ \\
       \bottomrule
    \end{tabularx}
  \end{center}
  \vspace{1em}
  \begin{itemize}
    \item $F(6, 28) = 6.026, p < 0.001$, with Wilcoxon tests significant when
    compared widths are between a $W \leq 32$ model and a $W > 32$ model.
    \item Best performer is always one of $W \in \{4, 8, 16\}$, even for ASR.
    \item $W = 128$ occasionally nears best performer, though not reliably.
    \item ABX performance does not map nicely to validation loss.
  \end{itemize}
}

\block{\uoftheader{Discussion and Conclusions}}{%
  \begin{itemize}
    \item Experiments lend strong support to our hypothesis that one can have
    ``too much'' context when pre-training for downstream tasks focusing on
    short-term phenomena.
    \item The preference for short windows in ASR may be due to a large,
    flexible dowstream model ($\sim 44$ million parameters).
  \end{itemize}
  \vspace{.5em}
  \coloredbox{%
    \textbf{Take-home:} rather than look for a ``universal context,'' we
    recommend learning heterogenous representations of multiple contexts.%
  }
}

% \block{\uoftheader{Motivations}}{%
%   \begin{enumerate}
%     \item \textbf{Ecological validity}
%           \begin{itemize}
%             \item Computer-Assisted Pronunciation Training (CAPT) systems often
%                   start with Automatic Speech Recognition (ASR)
%             \item Non-native ASR difficult
%             \item CAPT training commonly uses clean, read-and-record data
%             \item Read-and-record $\neq$ participating in CAPT
%             \item FAB data more applicable to downstream task $\to$ better
%                   training
%           \end{itemize}
%     \item \textbf{Teacher-driven labels}
%           \begin{itemize}
%             \item CAPT systems rely on intrinsic criterion
%                   of \textit{nativeness}
%                   \begin{itemize}
%                     \item Easy for engineers: more native $\approx$ more
%                           probable under ASR
%                     \item Problematic for educators: \textit{which} native?
%                           ``Good enough?''
%                   \end{itemize}
%             \item Solution: assume teacher knows best and mimic her feedback
%             \item FAB provides both intrinsic (nativeness) and extrinsic
%                   (teacher feedback) binary label sets
%           \end{itemize}
%     \item \textbf{Absolute beginner learners}
%           \begin{itemize}
%             \item Pronunciation training more impactful to beginner and
%                   intermediate learners than advanced ones (Mahdi and Al
%                   Khateeb, 2019)
%             \item Absolute (L2) beginners overlooked in existing corpora
%             \item FAB is the first corpus built from absolute beginner
%                   learners' speech
%           \end{itemize}
%   \end{enumerate}
% }

% \block{\uoftheader{Overview and collection}}{%
%   %
%   \begin{center}
%     \begin{tabular}{| r || l |}
%       \hline
%       Speakers                & 121 \\ \hline
%       Segmented utterances    & approx. 9000 \\ \hline
%       Word segments           & approx. 25000 \\ \hline
%       Binary labels           & approx. 17000 \\ \hline
%       Word segment duration   & approx. 3 hours \\ \hline
%       % Recording format        & 16kHz PCM16 mono WAV \\ \hline
%       % Transcription format    & \textit{CTM}, \textit{TextGrid},
%       %                           \textit{Transcriber} \\ \hline
%     \end{tabular}
%   \end{center}

%   \begin{itemize}
%     \item Collected over two experiments at University of Toronto
%     \item English-speaking participants knew little-to-no French
%     \item Paired role-play tasks, consistent with task-based pedagogy (Skehan,
%           2003)
%     \item Dialogue modelled by video
%     \item Participants rehearsed dialogue with small contextual changes
%     \item Participants took turns speaking into iPad app
%     \item App gave feedback, including accept/reject utterance
%     \item App actually controlled by expert in language learning
%     \item Beginners $+$ recall $+$ contextual inference $=$ very messy data!
%     \item Data from first experiment in minimal feedback partition; second in
%           explicit and read-and-record partitions
%     \item Nativeness labels in minimal feedback partition; teacher feedback
%           labels in explicit feedback partition
%   \end{itemize}
% }

% \column{0.33}

% \block{\uoftheader{Minimal feedback partition}}{%
%   %
%   \begin{center}
%     \begin{tabular}{| r || l |}
%       \hline
%       Speakers                        & 58 \\ \hline
%       Segmented utterances            & approx. 4000 \\ \hline
%       Word segments                   & approx. 12000 \\ \hline
%       More/less native labels         & 5767 \\ \hline
%       % First language                  & English(26), Portuguese(13),
%       %                                   Mandarin(4), Cantonese(4) \\ \hline
%       Fluency in French (1-5 asc.)    & 1(52), 2(8), 3(3) \\ \hline
%       % French experience               & None(16), Formal(16),
%       %                                   Incomplete(4), Informal(3) \\ \hline
%       % Median age                      & 23 \\ \hline
%       % Gender                          & Female(30), Male(28) \\ \hline
%       % Number languages fluent         & 2(26), 1(19), 3(11) \\ \hline
%     \end{tabular}
%     \hfil
%     \begin{tabular}{| r || c | c |}
%       \hline
%       ann. & $\kappa_1$ & $\kappa_2$ (no.) \\ %& more & less & contr. & prop. \\
%       \hline \hline
%       A & .16 & .48(168) \\ \hline %& .23 & .36 & .12 & .55 \\ \hline %corinne
%       B & .20 & .52(184) \\ \hline %& .29 & .41 & .9 & .10 \\ \hline %mei
%       C & .17 & .50(174) \\ \hline %& .29 & .37 & .13 & .10 \\ \hline %celine
%       D & .16 & .51(154) \\ \hline %& .31 & .23 & .14 & .25 \\ \hline %mimi
%    \end{tabular}
%   \end{center}

%   \begin{itemize}
%     \item Collected in first experiment (Robertson et al, 2016)
%     \item App feedback limited to accepting/rejecting utterances
%     \item More/less native word labels extracted by bucketed pairwise comparisons
%           %
%           \begin{itemize}
%             \item Four expert annotators compared pairs, winner the ``more
%                   native'' of two
%             \item 10 instances fully ranked counting wins, choose
%                   4\textsuperscript{th} and 6\textsuperscript{th} as boundaries
%             \item New word loses against both boundaries assigned ``less
%                   native'' label; if it wins both, it is assigned ``more
%                   native'' label
%             \item Inter-annotator agreement in above table; $\kappa_1$ is
%                   Cohen's Kappa including one-win-one-loss words; $\kappa_2$
%                   only win-win or loss-loss
%           \end{itemize}
%   \end{itemize}
% }

% \block{\uoftheader{Explicit feedback parition}}{%
%   %
%   \begin{center}
%     \begin{tabular}{| r || l |}
%       \hline
%       Speakers                        & 63 \\ \hline
%       Segmented utterances            & approx. 5000 \\ \hline
%       Word segments                   & approx. 13000 \\ \hline
%       Correct/mispronounced labels    & approx. 12000 \\ \hline
%       % First language                  & English(28), Mandarin(7),
%       %                                   Chinese\textsuperscript{*}(9),
%       %                                   Cantonese(3), Russian(3),
%       %                                   Spanish(2), Vietnamese(2),
%       %                                   Farsi(2), Korean(2),
%       %                                   Hindi(2), Malayalam(2) \\ \hline
%       Fluency in French (1-5 asc.)    & 1(59), 2(4) \\ \hline
%       % French experience               & None(37), Informal(8), Full(4)
%       %                                   \\ \hline
%       % Median age                      & 23 \\ \hline
%       % Gender                          & Female(32), Male(31) \\ \hline
%       % Number languages fluent         & 2(36), 3(14), 1(12) \\ \hline
%     \end{tabular}
%     %
%   \end{center}

%   \begin{itemize}
%     \item Collected in second experiment (Robertson et al, 2018)
%     \item Word-level feedback in addition to accept/reject: insertions,
%           deletions, mispronunciations, and \textit{recast} recordings
%     \item Expert feedback replaced with automated feedback in experimental
%           condition. See \textit{Challenges} section for results.
%     \item Correct/mispronounced labels extracted from teacher feedback
%   \end{itemize}
% }

% \block{\uoftheader{Read-and-record partition}}{%
%   %
%   \begin{center}
%     \begin{tabular}{| r || l |}
%       \hline
%       Speakers                        & 19 \\ \hline
%       Segmented utterances            & approx. 600 \\ \hline
%       Word segments                   & approx. 2000 \\ \hline
%       % Word segment duration           & 0.28 hours \\ \hline
%       % Total segments                  & 3686 \\ \hline
%       % First language                  & English(8), Mandarin(3) \\ \hline
%       Fluency in French (1-5 asc.)    & 1(18), 2(1) \\ \hline
%       % French experience               & None(7), Informal(4),
%       %                                   Formal(2) \\ \hline
%       % Median age                      & 23 \\ \hline
%       % Gender                          & Male(11), Female(8) \\ \hline
%       % Number languages fluent         & 2(11), 1(5), 3(3) \\ \hline
%     \end{tabular}
%   \end{center}

%   \begin{itemize}
%     \item Collected \textit{after} second experiment, time-permitting
%     \item Participants read aloud and recorded as many utterances as possible
%     \item Prompts were chosen for phonemic diversity
%   \end{itemize}
% }

% \note[width=.20\linewidth,targetoffsety=-3.5in,targetoffsetx=-0.01\linewidth]{
%   \large
%   \textbf{Freely avaliable} this year on U. of T.'s \textit{Dataverse}: \\
%   \url{https://dataverse.scholarsportal.info/dataverse/toronto}
% }

% \usenotestyle{Empty}

% \note[width=.10\linewidth,targetoffsety=-6in,targetoffsetx=-0.15\linewidth]{
%   \tiny
%   This research was funded by an Ontario Centres of Excellence Technical
%   Problem Solving grant in partnership with Speax Inc., and a Canada Graduate
%   Scholarship from the Natural Sciences and Engineering Research Council of
%   Canada.
% }

% \column{0.33}

% \block{\uoftheader{Results}}{%
%   \begin{itemize}
%     \item Performed on nativeness labels with manual word alignments
%     \item Pronunciation Error Detectors (PEDs) classify words as (non-)native:
%           \begin{itemize}
%             \item Goodness of Pronunciation (GOP, Witt and Young, 2000):
%                   empirically-tuned threshold of
%                   \begin{equation*}
%                     \log P_{\textrm{native}}(\textrm{expected words}|\textrm{audio}) -
%                     \max_{\text{any words}}
%                       \log P_{\textrm{native}}(\textrm{any words}|\textrm{audio})
%                   \end{equation*}
%             \item Four based on GMMs trained on audio segments (Franco et al,
%                   2014):
%                   \begin{itemize}
%                     \item GMM1: two separately-trained GMMs per word, native vs.
%                           non-native, with empirically-tuned threshold of
%                           \begin{equation*}
%                             \log P_{\textrm{word+native}}(\textrm{audio segment}) -
%                             \log P_{\textrm{word+non-native}}(\textrm{audio segment})
%                           \end{equation*}
%                     \item GMM2: same as GMM1, but GMMs MAP-adapted to
%                           Universal Background Model (UBM)
%                     \item GMM3: UBM MAP-adapted to each word instance $\to$
%                           extract GMM supervectors $\to$ linear SVM classifier
%                           on supervectors
%                     \item GMM4: linear combination of GMM2 $+$ GMM3
%                   \end{itemize}
%           \end{itemize}
%     \item Average 4-fold cross-validation with 3 partitioning strategies:
%           split by annotator (4FA); split by participant (4FP); split to
%           stratify database (4FS)
%   \end{itemize}
%   %
%   \begin{center}
%     \begin{tabular}{| r || c | c | c | c | c | c |}
%       \hline
%       \multirow{2}{*}{PED} & \multicolumn{2}{| c |}{4FA} & \multicolumn{2}{ c |}{4FP}
%         & \multicolumn{2}{ c |}{4FS} \\
%         &  Accuracy & $\kappa$ &  Accuracy & $\kappa$ &  Accuracy & $\kappa$ \\
%       \hline \hline
%       GOP &     62\% & .23      &     63\% & .25      &     63\% & .24      \\ \hline
%       GMM1&     63\% & .23      &     63\% & .25      &     66\% & .31      \\ \hline
%       GMM2&{\bf 68\%}&{\bf.34  }&{\bf 66\%}&{\bf.31  }&{\bf 71\%}&{\bf .41 }\\ \hline
%       GMM3&     65\% & .27      &     62\% & .23      &     67\% & .33      \\ \hline
%       GMM4&     64\% & .27      &     65\% & .29      &     70\% & .38      \\ \hline
%     \end{tabular}

%     {\small Results tuned on test set. See Robertson et al., 2016 for untuned.}
%   \end{center}
%   %
% }

% \block{\uoftheader{Challenges}}{%
%   %
%   \begin{minipage}{0.20\columnwidth}
%     \begin{itemize}
%       \item During explicit feedback experiment, pit ASR+GMM2 against
%             heuristic, rule-based PED
%       \item GMM2 tuned to segments and labels from pilot phase (no task
%             mismatch)
%     \end{itemize}
%   \end{minipage}
%   %
%   \hfil
%   %
%   \begin{minipage}{0.05\columnwidth}
%     \centering
%     \begin{tabular}{| r || c |}
%       \hline
%       PED         & $\kappa$ \\
%       \hline \hline
%       GMM2        & .19 \\ \hline
%       Heuristic   & \textbf{.34} \\ \hline
%     \end{tabular}
%   \end{minipage}
%   %
%   \begin{itemize}
%     \item On rejected utterances, heuristic picks ``hardest'' word to
%           pronounce, round-robins mispronounced word on successive rejections
%     \item Heuristic beats GMM2 in $\kappa \to$ \textbf{better results when mic
%           is turned off!}
%     \item Possible reasons:
%           %
%           \begin{itemize}
%             \item Non-native ASR difficult w/ little training data $\to$ bad
%                   alignments
%             \item Context effects: expert focuses on one word at a time,
%                   severity of mispronunciation, learner's motivation, etc.
%           \end{itemize}
%     \item Task easier than it \textit{should} be since teacher provides
%           transcription
%     \item \textbf{Need approach robust to misrecognized words, poor alignments,
%           and dialouge effects!}
%   \end{itemize}
% }

\end{columns}
\end{document}